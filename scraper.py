from typing import Dict, List, Optional
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup

Recipe = Dict[str, str]
RecipeList = List[Recipe]  # establish an alias for this list of dictionaries

class Scraper:
    bsep: str = '=' * 50
    sep: str = '-' * 50

    def __init__(self, data_name: str, recipe_list: Optional[RecipeList] = None, max_recipe_num: int = 5) -> None:
        """

        :param recipe_list: optional list of recipe url's to parse
        :param max_recipe_num: maximum number of recipes to scrape per category page
        """
        self.data_name = data_name
        if recipe_list is None:
            recipe_list = []
        self.recipe_list: RecipeList = recipe_list
        self.category_list: RecipeList = []
        self.max_recipe_num: int = max_recipe_num
        cols = ['category', 'recipe-name', 'recipe-url', 'prep-time', 'cook-time', 'total-time',
                'servings', 'calories', 'fat-g', 'carbs-g', 'protein-g', 'cholesterol-mg', 'sodium-mg',
                'ingredients', 'num_steps']
        self.df_recipes = pd.DataFrame(columns=cols)

    def get_list_of_categories(self, url: str) -> None:
        source = requests.get(url).text
        list_soup = BeautifulSoup(source, 'lxml')
        categories_html = list_soup.find('div', class_='grid slider').ul

        print(self.bsep)
        print('Finding List of Categories...')
        for category in categories_html.find_all('li'):  # <li> elements
            cat_name = category.a.span.text
            cat_url = category.a['href']
            self.category_list.append({'name': cat_name, 'url': cat_url})
            print('\t+{}'.format(cat_name))
        print('\nTotal of {} categories were found'.format(len(self.category_list)))

    def parse_category_list_for_recipes(self):
        print(self.bsep)
        for i, category in enumerate(self.category_list):
            print('\n', self.sep)
            print('+Finding recipes of type: {}'.format(category['name']))
            self._parse_category_for_recipes(category)

    def _parse_category_for_recipes(self, category: Recipe) -> None:
        time.sleep(1)
        cat_url = category['url']
        source = requests.get(cat_url).text
        cat_soup = BeautifulSoup(source, 'lxml')
        recipes = cat_soup.find_all('div', class_='fixed-recipe-card__info')
        for i, recipe in enumerate(recipes):
            if i >= self.max_recipe_num:
                break
            rec = {'category': category['name'], 'recipe-name': recipe.h3.a.span.text, 'recipe-url': recipe.h3.a['href']}
            self._parse_recipe(rec)

    def _parse_recipe(self, recipe: Recipe) -> None:
        print('\t-{}'.format(recipe['recipe-name']))
        # get html of recipe page
        time.sleep(1)
        source = requests.get(recipe['recipe-url']).text
        soup = BeautifulSoup(source, 'lxml')
        # parse for the remaining information
        times = soup.find('ul', class_='prepTime')
        try:
            for i, li in enumerate(times.find_all('li')):
                if i == 0:
                    continue
                elif i == 1:  #   prep-time
                    recipe['prep-time'] = li.span.text
                elif i == 2:  #   cook-time
                    recipe['cook-time'] = li.span.text
                elif i == 3:  #   total-time
                    recipe['total-time'] = li.span.text
        except AttributeError:
            recipe['prep-time'] = ''
            recipe['cook-time'] = ''
            recipe['total-time'] = ''

        #   servings
        try:
            recipe['servings'] = soup.find('div', class_='subtext').text
        except AttributeError:
            recipe['servings'] = ''
        # note that the servings number appears in an ng-binding span that is dynamically generated by angularJS
        #   calories
        try:
            recipe['calories'] = soup.find('span', class_='calorie-count').text
        except AttributeError:
            recipe['calories'] = ''

        #   fat-g
        try:
            recipe['fat-g'] = soup.find('span', itemprop='fatContent').text
        except AttributeError:
            recipe['fat-g'] = ''

        #   carbs-g
        try:
            recipe['carbs-g'] = soup.find('span', itemprop='carbohydrateContent').text
        except AttributeError:
            recipe['carbs-g'] = ''

        #   protein-g
        try:
            recipe['protein-g'] = soup.find('span', itemprop='proteinContent').text
        except AttributeError:
            recipe['protein-g'] = ''

        #   cholesterol-mg
        try:
            recipe['cholesterol-mg'] = soup.find('span', itemprop='cholesterolContent').text
        except AttributeError:
            recipe['cholesterol-mg'] = ''

        #   sodium-mg
        try:
            recipe['sodium-mg'] = soup.find('span', itemprop='sodiumContent').text
        except AttributeError:
            recipe['sodium-mg'] = ''

        #   ingredients
        ing_list: List[str] = []
        try:
            for li in soup.find_all('li', class_='checkList__line'):
                span = li.label.find('span', itemprop='recipeIngredient')
                if span is not None:
                    ing_list.append(span.text)
        except AttributeError:
            pass
        recipe['ingredients'] = ';'.join(ing_list)
        #   num_steps
        try:
            steps = soup.find('ol', itemprop='recipeInstructions').find_all('li', class_='step')
        except AttributeError:
            steps = []
        recipe['num_steps'] = str(len(steps))
        # Finally, add the dictionary to the recipe data frame
        self.df_recipes = self.df_recipes.append(recipe, ignore_index=True)

    def save_to_csv(self, directory_name: str) -> None:
        file_name = directory_name + self.data_name + '.csv'
        self.df_recipes.to_csv(file_name)




if __name__ == '__main__':
    scraper = Scraper('Breakfast', max_recipe_num=1)
    main_url = 'https://www.allrecipes.com/recipes/78/breakfast-and-brunch/?internalSource=hub%20nav&referringContentType=Recipe%20Hub&linkName=hub%20nav%20daughter&clickId=hub%20nav%202'
    scraper.get_list_of_categories(main_url)
    scraper.parse_category_list_for_recipes()
    scraper.save_to_csv('./data/')